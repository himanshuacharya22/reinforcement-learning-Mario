# ğŸ„ Super Mario Bros â€” Reinforcement Learning

Train a PPO agent to play Super Mario Bros using [Stable-Baselines3](https://stable-baselines3.readthedocs.io/).

## Features

- **Tuned PPO hyperparameters** â€” linear LR decay, optimized clip range, entropy coefficient
- **Multi-signal reward shaping** â€” forward progress, score, time penalty, death penalty, idle penalty, level-clear bonus
- **Professional preprocessing** â€” frame skip, grayscale, 84Ã—84 resize, 4-frame stack, channels-first CNN
- **COMPLEX_MOVEMENT action space** â€” 12 actions including run+jump combinations
- **TensorBoard logging** â€” monitor training in real time
- **Checkpoint & best-model saving** â€” never lose progress
- **CLI overrides** â€” tune any parameter without editing code
- **Separate evaluation script** â€” watch your agent play with episode statistics

## Quick Start

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Train

```bash
python train.py                          # Full training (2M steps)
python train.py --total_timesteps 500000 # Quick run
python train.py --num_envs 4             # Parallel environments
```

### 3. Monitor

```bash
tensorboard --logdir logs/
```

### 4. Evaluate

```bash
python evaluate.py                                            # Best model
python evaluate.py --model_path models/checkpoint_50000.zip   # Specific checkpoint
python evaluate.py --episodes 10                              # 10 episodes
```

### 5. Resume training

```bash
python train.py --resume_from models/checkpoint_100000.zip
```

## Project Structure

```
â”œâ”€â”€ config.py              # All hyperparameters & paths (dataclass-based)
â”œâ”€â”€ env_factory.py         # Environment construction pipeline
â”œâ”€â”€ train.py               # Training script with CLI args
â”œâ”€â”€ evaluate.py            # Evaluation & playback script
â”œâ”€â”€ callbacks.py           # Checkpoint & metric callbacks
â”œâ”€â”€ wrappers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ reward_wrapper.py  # Multi-signal reward shaping
â”‚   â”œâ”€â”€ skip_frame.py      # Frame skip with max-pooling
â”‚   â””â”€â”€ resize_obs.py      # Observation resizing (84Ã—84)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Configuration

All parameters are in `config.py`. Override any value via CLI flags:

| Flag | Default | Description |
|---|---|---|
| `--total_timesteps` | 2,000,000 | Total training steps |
| `--learning_rate` | 2.5e-4 | PPO learning rate |
| `--n_steps` | 512 | Rollout steps per update |
| `--batch_size` | 64 | Minibatch size |
| `--n_epochs` | 10 | PPO epochs per update |
| `--ent_coef` | 0.01 | Entropy coefficient |
| `--gamma` | 0.99 | Discount factor |
| `--num_envs` | 1 | Parallel environments |
| `--frame_skip` | 4 | Frames to skip |
| `--checkpoint_freq` | 25,000 | Steps between checkpoints |
| `--eval_freq` | 25,000 | Steps between evaluations |
| `--seed` | 42 | Random seed |
| `--resume_from` | â€” | Checkpoint path to resume |

## License

MIT
